\documentclass{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\pagestyle{fancy}
\fancyhf{}
\rhead{Rahul Shah}
\lhead{Machine Learning: Nanodegree}
\rfoot{\thepage}

\title{Introduction to Neural Networks}
\begin{document}
\maketitle
\section{What is a Neural Network?}
Neural Networks vaguely mimic how a brain works. The structure is essentially
like a perceptron. 
% Put a diagram of a perceptron here
This is just another way of looking at a linear model, with inputs $x_1, x_2,
x_3, \dots, x_n$ and output $f(\vec{x}, \vec{w}) = w_1x_1 + \dots + w_nx_n
= w \dot x$. 

A neural network is essentially layers upon layers of these models, 
where each edge in the graph represents a weight to multiply the input with. We
have layers upon layers of these nodes, essentially mapping the last layer's
output to another layer.

\begin{tikzpicture}

  [scale=.9, auto=center, every node/.style{circle,fill=blue!20}]

  \node (x0) at (1,1) {$x_0$}
  \node (x1) at (1,2) {$x_1$}
  \node (dots) at (1,3) {\dots}
  \node (xn) at (1,5) {$x_n$}
  \node (f) at (2, 3) {$f(\vec{x}, \vec{w})$}
  
  \path[every node/.style={font=\sffamily\small}]
    
    (x0) edge node {$w_0$} (f)
    (x1) edge node {$w_0$} (f)
    (xn) edge node {$w_0$} (f)

\end{tikzpicture}

\section{Error Functions}

\end{document}
