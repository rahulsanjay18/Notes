\documentclass{article}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[section]{placeins}
\usepackage{titlesec}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\pagestyle{fancy}
\fancyhf{}
\rhead{Rahul Shah}
\lhead{Machine Learning: Nanodegree}
\rfoot{\thepage}
\title{2.8 Model Evaluation Metrics}
\begin{document}
\maketitle
\section{Motivation}
How do we evaluate models? What metrics do we use?
\section{Dealing with Training Data}
We need to make sure our models are not overfitting, so we need to take our training data and split it into training data and testing data. Training data will be used to actually train the model, and the test data will be used to verify that the model is accurate. We can tell if the model is accurate if the test data's margin of error and the training data's margin of error are the same. Typically we split 75:25 or 80:20 training:testing.
\section{Confusion matrix}
When evaluatiing effectiveness of a classification algorithm, we can put results in a table.\\
\begin{table}[]
\begin{tabular}{cccc}
 & Guessed Class 1  & ... & Guessed Class N \\
Actual Class 1 &  &  &  \\
... &  &  &  \\
Actual Class N &  &  & 
\end{tabular}
\end{table}
\\

Basically we use this to evaluate Type I (False Positive) and Type II (False Negative) error. We measure accurracy via a standard average:\\
$\frac{\Sigma True guesses}{Total}$\\

Depending on your problem, you want to punish false positives or false negatives more, and we can use the following measurements to see what proportion of positive classifications are Type 1 or Type 2 errors:\\
Recall: False positives are fine, false negatives are not: $\frac{True(+)}{True(+) + False(-)}$ (Ex. This metric is useful when testing if someone has a virus that is particularly contagious)
Precision: False negatives are fine, false positives are not: $\frac{True(+}{True(+) + False(+)$ (Ex. This metric is useful when classifying spam emails)\\
\section{The F-beta score}
To put these two measurements together to a single measurement, we use the f-beta score, which tells us a sort of weighted average between the two.\\
$F_{\beta} = (1 + \beta^2)\frac{P * R}{\beta^2 * P + R}$
\\
We choose beta closer to zero when we want to emphasize precision, and we choose beta closer to infinity when we want to emphasize recall, with the middle point being 1. We don't use arithmetic mean because if we have zero for one of the metrics and one hundred for another one of the metrics, the arithmetic mean becomes half, which is inaccurate. We want a low precision or recall to make the entire score low, because ultimately, we want both precision and recall to be high.

\section{Preventing Over/Underfitting}
We want to make sure that our test set is truly for testing accuracy, not for checking which model we want to use. So, we split the data into yet another portion, cross validation, which acts as a sort of test set but for choosing a model. So, to clarify, our data is used in the following ways:\\
Our data sets are the following:
\begin{itemize}
	\item input data coupled with the correct output (used before the final model is determined)
	\item input data with no output (used on the final model after it is determined)
\end{itemize}\\

We are only concerned with the first bullet, obviously. When training the data, we choose multiple different models to see which model represents the data the best. While training each model and trying to figure out which model works best and what weights to use in the model, we do the following:\\
\begin{itemize}
	\item Training phase: We use part of the data from the first bullet of the last list and train each of our potential models (50\%)
	\item Validation phase: We take a different part of the data and use it to test each of our models to see which one is the best model (25\%). We also use this data to estimate model properties. This set also tunes hyperparameters.
	\item Test phase: choose the model that performed best in the validation phase, and check it's accuracy.(25\%)
\end{itemize}

You only need this validation set when you are trying to see which model perform the best. Using the test set as validation is often bad because the test set rror of the final model will underestimate the true testing error, sometimes by a lot.

\subsection{K-Fold Cross Validation}
The method shown above implies that you need even more data to get a good model. Remember the model trains itself by the data we give to it, and the order by which we give the data (training and testing set) changes our final parameters. We use k-fold cross validation to train and test different splits of the data, to get a more accurate model. Basically, we take our data, randomly split it into k equal sized parts, use k-1 of those parts as training data, and 1 part as testing data. Then do that k times. We typically use $k=10$. The splitting can be done in many different ways randomly, but here are a few common methods:
\begin{itemize}
	\item stratified k-fold cross validation: the partitions are selected so the mean response value is approximately equal.
	\item repeated cross validation: split into k partitions several times, and average the performance over several runs
\end{itemize}

\subsection{Grid Search}
Often, we don't know what hyperparameters will help train the model best, so when we pick the model we want, we can use a grid search and basically test each combination of potential hyperparameters against a validation set. 


\end{document}
