\documentclass{article}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[section]{placeins}
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\pagestyle{fancy}
\fancyhf{}
\rhead{Rahul Shah}
\lhead{Machine Learning: Nanodegree}
\rfoot{\thepage}
\title{Supervised Learning: Linear Regression}
\begin{document}
\maketitle
\section{Motivation}
% Talk about the housing problem and trying to predict the next house in
Imagine you are trying to predict the prices of houses. The only data you have available to you are the prices of a bunch of houses, and the square footage of all the houses. If given another house, alongside the square footage of said house, can you predict the price of the house?

If we assume that increasing square footage is approximately proportional to the price by a constant (the data is approximately linear), we can plot each point $P_i = (x, y) = (sq ft, \$\$)$ in a scatter plot and find a line of best fit to the data.

\section{Fitting a Line}
% If we know the thing is linear, then we can use a linear model to predict it
For any set of data \emph{assuming the data is approximately linear}, we can use a linear model to predict the output of any new data that comes in. 
% make sure you mention the linear model we are using and what the coefficients mean
As a reminder, the linear model we are working with (for now) is:\newline $\hat{y} = \beta_1x + \beta_2$\newline where \newline $\hat{y} :=$ Predicted output of the linear model.\newline Note that if we increase $\beta_1$, we rotate the line CCW, if we decrease $\beta_1$, we rotate the line CW, and increasing/decreasing $\beta_2$ pulls up/down the line.
\section{Moving a Line Towards a Point}
% This is how we can approximate a line movement
Let's assume the line we have to work with is: $y(x) = \beta_1x + \beta_2$ ($y(x)$ will be treated as y unless explicitly used as a function), where $\beta_1, \beta_2$ are arbitrary constants, like a really bad guess as to what the actual line would be. Place a point reasonably far away from the line, $P = (p, q)$. How do we, using the data that we have, move the line such that it gets closer to the point? 
\subsection{Absolute Trick}
The Absolute Trick is as such: Every iteration, we change $\beta_1$ by the value $p$ (the x coordinate), and change the value $\beta_2$ by the value 1 (we will get to cases where we increase/decrease in a second). So our model transforms to:\\
$y = \beta_1x+\beta_2 \rightarrow y = (\beta_1 \pm p)x+(\beta_2 \pm 1)$, where each $\pm$ is independent of one another\\
If we continue under the assumption that the point is far away, we might get a good approximation. In all likelihood, however, we are likely to overshoot. So instead we iterate over this multiple times, taking steps towards the right solution. We can do this by multiplying $p$ and 1 by $\alpha$, the learning rate. So our formula becomes:\\
$y = \beta_1x+\beta_2 \rightarrow y = (\beta_1 \pm p\alpha)x+(\beta_2 \pm \alpha)$\\
To determine whether to use + or -, we need to look at how we want the line to move. For $\beta_1$, we see which direction (CCW or CW) we should rotate the line to reach the point (smallest angle to sweep across so the line meets the point). For $\beta_2$, do the same thing, but this time your distance is up/down.
\subsection{Square Trick}
The above trick works to get near to the point, but we never factor in the y distance from the line (other than to determine the sign). What if we adjust $\beta_1$ and $\beta_2$ the same way as above, but also account for the vertical distance from the point to the line? We can do this by multiplying our $p\alpha$ and $\alpha$ term by $q-\hat{q}$, where $\hat{q} = y(p)$. So our formula for the square trick becomes:\\
$y = \beta_1x+\beta_2 \rightarrow y = (\beta_1 + p\alpha(q-\hat{q}))x+(\beta_2 + \alpha(q-\hat{q}))$\\
Note that we don't need to use $\pm$ anymore because multiplying by $q-\hat{q}$ means that if our point is below our line, this difference will yield a negative number,  and if the point is above our line, the difference will yield a positive number.
\subsection{Why not just do $\frac{q}{p}x$ as our line?}
Obviously these tricks are pretty bad standalone operations, but their power is in the fact that they aren't exact. When we do linear regression, we're modifying our line of best fit so it coincides with all the data, and, unless all of our data points are co-linear (HIGHLY unlikely!), we want our line of best fit to be close to all of our data points. Think of each point pulling towards a line, the farther away the point from the line, the harder it pulls the line towards it.

\section{What is the Line of Best Fit}
% Note that if we define error as the difference between our predicted y from our actual point, our best fit line is essentially the placement of the line such that the sum of all errors is at its smallest
Now that we have an iterative approach to pulling a line closer to a point, we need to define what a line of best fit is. If we define error (roughly) as the difference between our predicted y for each x value in our data, $\hat{y} = y(x)$, and the actual y value from each data point in our value, then our line of best fit is such that $\sum f(|y-\hat{y}|)$, such that $f$ is a monotonic function, is at a minimum.

\section{Linear Least Squares}
% Note on Linear Least Squares (matrix solution), and how, though it does solve the problem completely, it also involves inverting a matrix, which can be computationally taxing.

Turns out, finding the exact line of best fit for a set of points $(x_i, y_i)$ is an already solved problem, with a nice, closed form solution. Let our linear model be $\beta_1 x + \beta_0 = y$. If we consider the linear system:\\
$X\beta = y$\\
Where:\\
$X :=$ the overdetermined matrix
$\begin{bmatrix}
	1 & x_1\\
	\vdots & \vdots\\
	1 & x_n
\end{bmatrix}$\\
$y:=$ the vector of y values associated with each x in the data\\
$\beta :=$ the vector of beta constants in the linear system (from this model, it's $\beta_1$ and $\beta_0$)\\
We have an overdetermined system that does not have exact solutions for $\beta_1, \beta_0$. Since we need a square matrix were $X$ is, so we can solve it, we can multiply both sides by $X^T$ to get:\\
$X^TX\beta=X^Ty$\\
Solving for $\beta$, we get:\\
$\beta=(X^TX)^{-1}y$, where $\beta$ is guaranteed to be such that our model $\beta_1 x + \beta_0 = y$ minimizes the error.

%NEED TO CHECK THIS THERE IS A WAY BUT I DONT WANT TO DO THIS
%This can even be generalized, if we have a model $y = \beta_0 + \sum_{1}^{n} beta_if_i(\vec{x})$, where each $f$ is a polynomial, we can make the entries of each row of our matrix the functions $f_i(\vec{x})$, with every row in the matrix being one point from the input data $\vec{x} = x_1, ..., x_n$\\

Unfortunately, this linear least squares approach is rarely used. This is because the formula $\beta=(X^TX)^{-1}y$ means that we need to calculate the inverse of a matrix, which is an $O(n^3)$ operation. 

% So, we take an iterative approach, (mention the mountain thing, and how, to get to the bottom, you just go where it falls off the most)

Since we can't solve this exactly, we need to get an approximation of our answer via iterative methods. 

\section{Error Functions}
Since our method must be iterative and will not give us the exact answer, it would be useful to have an idea of what our error for any given model is. 
\subsection{Mean Approximation Error (MAE)}
MAE $=\frac{1}{m}\sum |y_i - \hat{y}(x_i)$\\
We use absolute value so we do not have negative differences accidentally canceling positive ones. It's also a metric.
\subsection{Mean Squared Error (MSE)}
MSE $=\frac{1}{2m}\sum (y_i - \hat{y}(x_i))^2$\\
The added $\frac{1}{2} is a constant added for convienience.$

\section{Gradient Descent}
% Gradient is basically just the slope of the moutain at any point around you
Since our goal is to minimize error, why not just change $\beta_0, \beta_1$ in the direction of the greatest decrease of the error function, multiplied by a small number (so we don't overshoot)? That is, every iteration,\\
$\beta_i = \beta_i - \alpha\frac{\partial}{\partial \beta_i} Error = \beta_i - \alpha\frac{\partial Error}{\partial y_i}\frac{\partial y_i}{\partial x_i}$\\
If we apply this to our MAE and MSE square functions, we essentially get the Absolute and Square Trick, respectively


\subsection{Types of Gradient Descent}
% Batch, Stochastic, and Mini-Batch
% First two are basically terrible
% give pseudocode for mini-batch
Batch - compute the average change in $\beta$ using all data points, update $\beta$, and iterate over that
Stochastic - compute the change in $\beta$ using one data point, updating $\beta$, then do another, etc.
Mini-Batch - Split your data into small, even groups, and compute the change, and iterate over that.

Batch and Stochastic are pretty slow, so we do mini batch typically
% Can we make b just part of the matrix
\begin{algorithm}
	\caption{Gradient Descent- Mini Batch with MSE}
	\begin{algorithmic}
		\Procedure{Step}{$X$, $\beta$, $y$, $b$, $\alpha$}\\
			 \Comment $X$ the matrix containing loaded input data, $\beta$ the current values of our model's coefficients, $b$ current value of the regression intercept ($\beta_0$), $\alpha$ the step size. 
			 
			 $\hat{y} = XW + b$
			 $error = y - \hat{y}$
			 
			 $W += \alpha\times error\times X$ \Comment this is just the derivative (negative is removed)
			 $b += \alpha * \sum error_i$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\section{Generalizing to Higher Dimensions}
% Absolutely give matrix translations here or else nothing makes any goddamn sense

For an input vector $\vec{x} = x_1, ..., x_n$, we get an $n-1$ dimensional hyperplane.

\section{Warnings}
% Just copy whats on the Udacity site basically

\section{Polynomial Regression}
Basically, you can fit your model to any polynomial, so long as you treat each power of each input variable as another term on your x vector. Meaning, to model a quadratic,, you need rows of length 3 (const, power 1, power 2)

\section{Regularization}
Natrually we can fit multple models to the same set of data. We can also minimize error by just doing linear least squares to a polynomial of an arbitrarily high power, but this makes an extremely computationally complex problem to solve. We also end up with a model that does not generalize to new data. To account for that, we regularize the model by summing up the elements in $\beta$ and tacking that onto our total error. Models that do not generalize well will have a much larger regularization parameter. 

Sometimes, we want our model to be more accurate (less actual error) and are okay if the model is complex, so we multiply this regularization parameter by $\lambda$ to lower the error "punishment". If we can tolerate higher errors & need simplicity (perhaps operating on a lot of data), we use a higher $\lambda$ on our regularization parameter. \\

2 kinds of regularization:\\
L1: take the L1 distance of the coefficients\\
L2:    "     L2             "
\section{Feature Scaling}
When we implement regularization or are using a distance based metric, we need to scale the range of our input vector values so that the penalty for error is uniform, and you reach convergence faster. Essentially what you want to do is to make each feature (each $x_i$ for each piece of data) and make it so its mean is 0 and standard deviation is 1. You can also just normalize the data from 0 to 1 or standardize the values such that each value is a z-score
\section{External Resources}
%no later i hate this
\end{document}